 Outlier Engineering


An outlier is a data point which is significantly different from the remaining data. 
“An outlier is an observation which deviates so much from the other observations as to 
arouse suspicions that it was generated by a different mechanism.” 
[D. Hawkins. Identification of Outliers, Chapman and Hall , 1980].

Statistics such as the mean and variance are very susceptible to outliers. In addition, 
**some Machine Learning models are sensitive to outliers** which may decrease their performance. 
Thus, depending on which algorithm we wish to train, we often remove outliers from our variables.

We discussed in section 3 of this course how to identify outliers. In this section, we we discuss 
how we can process them to train our machine learning models.


 How can we pre-process outliers?

- Trimming: remove the outliers from our dataset
- Treat outliers as missing data, and proceed with any missing data imputation technique
- Discrestisation: outliers are placed in border bins together with higher or lower values of 
the distribution
- Censoring: capping the variable distribution at a max and / or minimum value

**Censoring** is also known as:

- top and bottom coding
- winsorization
- capping


## Censoring or Capping.

**Censoring**, or **capping**, means capping the maximum and /or minimum of a distribution at an 
arbitrary value. On other words, values bigger or smaller than the arbitrarily determined ones are 
**censored**.

Capping can be done at both tails, or just one of the tails, depending on the variable and the user.

Check [pydata](https://www.youtube.com/watch?v=KHGGlozsRtA), by Soledad Galli, for an example of 
capping used in a finance company.

The numbers at which to cap the distribution can be determined:

- arbitrarily
- using the inter-quantal range proximity rule
- using the gaussian approximation
- using quantiles


### Advantages

- does not remove data

### Limitations

- distorts the distributions of the variables
- distorts the relationships among variables


## In this Demo

We will see how to perform capping with the inter-quantile range proximity rule

## Important

When doing capping, we tend to cap values both in train and test set. It is important to remember 
that the capping values MUST be derived from the train set. And then use those same values to cap 
the variables in the test set

I will not do that in this demo, but please keep that in mind when setting up your pipelines